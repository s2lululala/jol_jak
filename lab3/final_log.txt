Script started on 2021-05-15 14:01:50+0000
]0;root@agora-desktop: /jetson-inference/temp/lab3root@agora-desktop:/jetson-inference/temp/lab3# exit[2Plspython sample.py ls[Kpscd temp/lab3_noURL/ls[Kcd temp/lab3_noURL/ps[Klspython sample.py ls[Kexit[Kexit[2Plspython sample.py ls[Kexit[Kpython3 ma ethod2.py 
[TensorRT] WARNING: Tensor DataType is determined at build time for tensors not marked as input or output.
[TensorRT] VERBOSE: Applying generic optimizations to the graph for inference.
[TensorRT] VERBOSE: Original: 259 layers
[TensorRT] VERBOSE: After dead-layer removal: 259 layers
[TensorRT] VERBOSE: Fusing [CONSTANT #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with [SHUFFLE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with [SHUFFLE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) with [SHUFFLE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) with [SHUFFLE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [SHUFFLE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [SHUFFLE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [SHUFFLE #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) with [SHUFFLE #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [SHUFFLE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [SHUFFLE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [SHUFFLE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #12] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [SHUFFLE #12] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [SHUFFLE #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) with [SHUFFLE #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #20] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #20] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [SHUFFLE #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [SHUFFLE #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [SHUFFLE #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [SHUFFLE #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #26] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [SHUFFLE #26] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [SHUFFLE #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) with [SHUFFLE #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [SHUFFLE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [SHUFFLE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [SHUFFLE #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [SHUFFLE #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [SHUFFLE #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #34] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [SHUFFLE #34] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONSTANT #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) with [SHUFFLE #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [SHUFFLE #36] torch.Tensor.reshape(tensor(shape=[1, 1280, 1, 1], dtype=torch.float32), 1, -1) with [SHUFFLE #37] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32))
[TensorRT] VERBOSE: Removing [SHUFFLE #36] torch.Tensor.reshape(tensor(shape=[1, 1280, 1, 1], dtype=torch.float32), 1, -1) + [SHUFFLE #37] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32))
[TensorRT] VERBOSE: After Myelin optimization: 222 layers
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) with scale [SCALE #1] torch.nn.functional.batch_norm(tensor(shape=[1, 32, 112, 112], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with scale [SCALE #2] torch.nn.functional.batch_norm(tensor(shape=[1, 32, 112, 112], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with scale [SCALE #3] torch.nn.functional.batch_norm(tensor(shape=[1, 16, 112, 112], dtype=torch.float32), tensor(shape=[16], dtype=torch.float32), tensor(shape=[16], dtype=torch.float32), tensor(shape=[16], dtype=torch.float32), tensor(shape=[16], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) with scale [SCALE #4] torch.nn.functional.batch_norm(tensor(shape=[1, 96, 112, 112], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) with scale [SCALE #5] torch.nn.functional.batch_norm(tensor(shape=[1, 96, 56, 56], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) with scale [SCALE #6] torch.nn.functional.batch_norm(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) with scale [SCALE #7] torch.nn.functional.batch_norm(tensor(shape=[1, 144, 56, 56], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with scale [SCALE #8] torch.nn.functional.batch_norm(tensor(shape=[1, 144, 56, 56], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with scale [SCALE #9] torch.nn.functional.batch_norm(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), tensor(shape=[24], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #10] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) with scale [SCALE #10] torch.nn.functional.batch_norm(tensor(shape=[1, 144, 56, 56], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with scale [SCALE #11] torch.nn.functional.batch_norm(tensor(shape=[1, 144, 28, 28], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), tensor(shape=[144], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) with scale [SCALE #12] torch.nn.functional.batch_norm(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) with scale [SCALE #13] torch.nn.functional.batch_norm(tensor(shape=[1, 192, 28, 28], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with scale [SCALE #14] torch.nn.functional.batch_norm(tensor(shape=[1, 192, 28, 28], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with scale [SCALE #15] torch.nn.functional.batch_norm(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #16] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) with scale [SCALE #16] torch.nn.functional.batch_norm(tensor(shape=[1, 192, 28, 28], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #17] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with scale [SCALE #17] torch.nn.functional.batch_norm(tensor(shape=[1, 192, 28, 28], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #18] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with scale [SCALE #18] torch.nn.functional.batch_norm(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), tensor(shape=[32], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #19] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) with scale [SCALE #19] torch.nn.functional.batch_norm(tensor(shape=[1, 192, 28, 28], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with scale [SCALE #20] torch.nn.functional.batch_norm(tensor(shape=[1, 192, 14, 14], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), tensor(shape=[192], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) with scale [SCALE #21] torch.nn.functional.batch_norm(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with scale [SCALE #22] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #23] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #24] torch.nn.functional.batch_norm(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #25] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with scale [SCALE #25] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #26] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #26] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #27] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #27] torch.nn.functional.batch_norm(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #28] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with scale [SCALE #28] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #29] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #29] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #30] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #30] torch.nn.functional.batch_norm(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), tensor(shape=[64], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #31] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with scale [SCALE #31] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #32] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #32] torch.nn.functional.batch_norm(tensor(shape=[1, 384, 14, 14], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), tensor(shape=[384], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with scale [SCALE #33] torch.nn.functional.batch_norm(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) with scale [SCALE #34] torch.nn.functional.batch_norm(tensor(shape=[1, 576, 14, 14], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with scale [SCALE #35] torch.nn.functional.batch_norm(tensor(shape=[1, 576, 14, 14], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with scale [SCALE #36] torch.nn.functional.batch_norm(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #37] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) with scale [SCALE #37] torch.nn.functional.batch_norm(tensor(shape=[1, 576, 14, 14], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #38] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with scale [SCALE #38] torch.nn.functional.batch_norm(tensor(shape=[1, 576, 14, 14], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #39] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with scale [SCALE #39] torch.nn.functional.batch_norm(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), tensor(shape=[96], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #40] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) with scale [SCALE #40] torch.nn.functional.batch_norm(tensor(shape=[1, 576, 14, 14], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with scale [SCALE #41] torch.nn.functional.batch_norm(tensor(shape=[1, 576, 7, 7], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), tensor(shape=[576], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) with scale [SCALE #42] torch.nn.functional.batch_norm(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) with scale [SCALE #43] torch.nn.functional.batch_norm(tensor(shape=[1, 960, 7, 7], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with scale [SCALE #44] torch.nn.functional.batch_norm(tensor(shape=[1, 960, 7, 7], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with scale [SCALE #45] torch.nn.functional.batch_norm(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #46] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) with scale [SCALE #46] torch.nn.functional.batch_norm(tensor(shape=[1, 960, 7, 7], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #47] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with scale [SCALE #47] torch.nn.functional.batch_norm(tensor(shape=[1, 960, 7, 7], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #48] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with scale [SCALE #48] torch.nn.functional.batch_norm(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), tensor(shape=[160], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #49] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) with scale [SCALE #49] torch.nn.functional.batch_norm(tensor(shape=[1, 960, 7, 7], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #50] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with scale [SCALE #50] torch.nn.functional.batch_norm(tensor(shape=[1, 960, 7, 7], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), tensor(shape=[960], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with scale [SCALE #51] torch.nn.functional.batch_norm(tensor(shape=[1, 320, 7, 7], dtype=torch.float32), tensor(shape=[320], dtype=torch.float32), tensor(shape=[320], dtype=torch.float32), tensor(shape=[320], dtype=torch.float32), tensor(shape=[320], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: Fusing convolution weights from [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) with scale [SCALE #52] torch.nn.functional.batch_norm(tensor(shape=[1, 1280, 7, 7], dtype=torch.float32), tensor(shape=[1280], dtype=torch.float32), tensor(shape=[1280], dtype=torch.float32), tensor(shape=[1280], dtype=torch.float32), tensor(shape=[1280], dtype=torch.float32), False, 0.1, 1e-05)
[TensorRT] VERBOSE: After scale fusion: 170 layers
[TensorRT] VERBOSE: Fusing [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) with [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) with [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) with [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) with [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) with [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) with [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) with [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #10] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) with [RELU #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #10] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [ELEMENTWISE #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) with [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) with [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #16] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) with [RELU #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #16] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #17] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [RELU #12] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #17] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #12] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #18] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #15] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #19] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) with [RELU #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #19] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [ELEMENTWISE #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) with [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #25] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with [RELU #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #25] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #26] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [RELU #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #26] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #27] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #23] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #28] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with [RELU #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #28] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #29] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [RELU #20] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #29] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #20] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #30] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #26] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #31] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) with [RELU #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #31] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #32] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [RELU #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #32] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) with [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #37] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) with [RELU #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #37] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #38] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [RELU #26] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #38] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #26] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #39] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #34] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #40] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) with [RELU #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #40] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [ELEMENTWISE #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) with [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) with [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #46] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) with [RELU #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #46] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #40] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #47] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [RELU #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #47] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #41] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #48] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #42] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #49] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) with [RELU #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #49] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #43] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #50] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [RELU #34] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #50] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #34] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #44] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) with [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: Fusing [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) with [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32))
[TensorRT] VERBOSE: After vertical fusions: 90 layers
[TensorRT] VERBOSE: After final dead-layer removal: 55 layers
[TensorRT] VERBOSE: After tensor merging: 55 layers
[TensorRT] VERBOSE: After concat removal: 55 layers
[TensorRT] VERBOSE: Graph construction and optimization completed in 0.215218 seconds.
[TensorRT] VERBOSE: Constructing optimization profile number 0 [1/1].
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,224,50176,150528) -> Float(1,112,12544,401408) ***************
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 3.97495
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 6.34646
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 12.2104
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 1.21323
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 1.24177
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 2.49479
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 0.826198
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.779583
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 2.42297
[TensorRT] VERBOSE: Fastest Tactic: -3456450830548107839 Time: 0.779583
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 4.18917
[TensorRT] VERBOSE: Tactic: 2 time 5.12979
[TensorRT] VERBOSE: Tactic: 5 time 45.8132
[TensorRT] VERBOSE: Tactic: 57 time 1.09885
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 1.09885
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3456450830548107839
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,112,12544,401408) -> Float(1,112,12544,401408) ***************
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 8.06099
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 12.8022
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 23.94
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 11.7311
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 12.5825
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 25.3892
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 8.24656
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 9.58443
[TensorRT] VERBOSE: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 24.4683
[TensorRT] VERBOSE: Fastest Tactic: 1062367460111450758 Time: 8.06099
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 1.43391
[TensorRT] VERBOSE: Tactic: 2 time 1.43021
[TensorRT] VERBOSE: Tactic: 5 time 152.685
[TensorRT] VERBOSE: Tactic: 6 time 18.9172
[TensorRT] VERBOSE: Tactic: 57 time 1.39911
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 1.39911
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.566042
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.566042
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,112,12544,401408) -> Float(1,112,12544,200704) ***************
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.732031
[TensorRT] VERBOSE: Tactic: 655359 time 1.44396
[TensorRT] VERBOSE: Tactic: 786431 time 0.901459
[TensorRT] VERBOSE: Tactic: 851967 time 2.40089
[TensorRT] VERBOSE: Tactic: 1179647 time 1.27521
[TensorRT] VERBOSE: Tactic: 1310719 time 1.20099
[TensorRT] VERBOSE: Tactic: 1376255 time 0.670677
[TensorRT] VERBOSE: Tactic: 1441791 time 1.70589
[TensorRT] VERBOSE: Tactic: 1507327 time 2.0926
[TensorRT] VERBOSE: Tactic: 1638399 time 0.987604
[TensorRT] VERBOSE: Tactic: 1835007 time 0.903177
[TensorRT] VERBOSE: Tactic: 1900543 time 1.65031
[TensorRT] VERBOSE: Tactic: 2097151 time 1.18182
[TensorRT] VERBOSE: Tactic: 2162687 time 0.707865
[TensorRT] VERBOSE: Tactic: 2293759 time 0.658437
[TensorRT] VERBOSE: Tactic: 2359295 time 1.05479
[TensorRT] VERBOSE: Tactic: 2686975 time 0.692969
[TensorRT] VERBOSE: Tactic: 3080191 time 1.81901
[TensorRT] VERBOSE: Tactic: 3342335 time 1.99266
[TensorRT] VERBOSE: Tactic: 3407871 time 0.869583
[TensorRT] VERBOSE: Tactic: 3538943 time 0.917135
[TensorRT] VERBOSE: Tactic: 3670015 time 1.20807
[TensorRT] VERBOSE: Tactic: 3932159 time 1.42073
[TensorRT] VERBOSE: Tactic: 3997695 time 0.95974
[TensorRT] VERBOSE: Tactic: 4063231 time 2.40865
[TensorRT] VERBOSE: Tactic: 4194303 time 0.864843
[TensorRT] VERBOSE: Tactic: 4259839 time 1.14172
[TensorRT] VERBOSE: Tactic: 4325375 time 0.851094
[TensorRT] VERBOSE: Tactic: 4521983 time 0.63125
[TensorRT] VERBOSE: Tactic: 4587519 time 0.938854
[TensorRT] VERBOSE: Tactic: 4653055 time 1.66302
[TensorRT] VERBOSE: Tactic: 4915199 time 0.902447
[TensorRT] VERBOSE: Tactic: 4980735 time 0.766719
[TensorRT] VERBOSE: Tactic: 5177343 time 1.25734
[TensorRT] VERBOSE: Tactic: 5242879 time 0.957187
[TensorRT] VERBOSE: Tactic: 5373951 time 1.35807
[TensorRT] VERBOSE: Tactic: 5439487 time 1.34115
[TensorRT] VERBOSE: Tactic: 5570559 time 2.05776
[TensorRT] VERBOSE: Tactic: 5636095 time 2.41302
[TensorRT] VERBOSE: Tactic: 5701631 time 0.726407
[TensorRT] VERBOSE: Tactic: 5767167 time 2.2376
[TensorRT] VERBOSE: Tactic: 5832703 time 0.856354
[TensorRT] VERBOSE: Tactic: 5898239 time 1.19151
[TensorRT] VERBOSE: Tactic: 6029311 time 0.715365
[TensorRT] VERBOSE: Tactic: 6225919 time 0.933125
[TensorRT] VERBOSE: Tactic: 6291455 time 1.27203
[TensorRT] VERBOSE: Tactic: 6422527 time 1.57088
[TensorRT] VERBOSE: Tactic: 6750207 time 0.890365
[TensorRT] VERBOSE: Tactic: 6815743 time 1.2313
[TensorRT] VERBOSE: Tactic: 6946815 time 1.3274
[TensorRT] VERBOSE: Tactic: 7012351 time 1.19979
[TensorRT] VERBOSE: Tactic: 7077887 time 0.90375
[TensorRT] VERBOSE: Tactic: 7143423 time 1.00521
[TensorRT] VERBOSE: Tactic: 7208959 time 0.95875
[TensorRT] VERBOSE: Tactic: 7340031 time 1.12755
[TensorRT] VERBOSE: Tactic: 7405567 time 1.24125
[TensorRT] VERBOSE: Tactic: 7536639 time 0.939896
[TensorRT] VERBOSE: Tactic: 7602175 time 0.888073
[TensorRT] VERBOSE: Tactic: 7733247 time 1.11297
[TensorRT] VERBOSE: Tactic: 7798783 time 0.899167
[TensorRT] VERBOSE: Tactic: 8191999 time 1.00568
[TensorRT] VERBOSE: Tactic: 8257535 time 0.901667
[TensorRT] VERBOSE: Tactic: 8323071 time 0.922552
[TensorRT] VERBOSE: Tactic: 8650751 time 0.958229
[TensorRT] VERBOSE: Tactic: 8716287 time 1.21271
[TensorRT] VERBOSE: Tactic: 9109503 time 1.16516
[TensorRT] VERBOSE: Tactic: 9568255 time 0.891719
[TensorRT] VERBOSE: Tactic: 9895935 time 0.864532
[TensorRT] VERBOSE: Tactic: 10223615 time 0.696354
[TensorRT] VERBOSE: Tactic: 10354687 time 0.932656
[TensorRT] VERBOSE: Tactic: 10551295 time 0.68099
[TensorRT] VERBOSE: Tactic: 10747903 time 1.07641
[TensorRT] VERBOSE: Tactic: 10944511 time 0.769219
[TensorRT] VERBOSE: Fastest Tactic: 4521983 Time: 0.63125
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.382917
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 1.05187
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.498698
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.9775
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.519375
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.330781
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.338698
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 1.00177
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.529115
[TensorRT] VERBOSE: Fastest Tactic: -6576203419454146580 Time: 0.330781
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.568021
[TensorRT] VERBOSE: Tactic: 2 time 0.809271
[TensorRT] VERBOSE: Tactic: 5 time 1.58187
[TensorRT] VERBOSE: Tactic: 57 time 0.466823
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.466823
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6576203419454146580
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,112,12544,200704) -> Float(1,112,12544,1204224) ***************
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.808281
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.802708
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.773698
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.835364
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.810104
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.810416
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.818385
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.843541
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.775
[TensorRT] VERBOSE: Fastest Tactic: 5137655947464784826 Time: 0.773698
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 2.97724
[TensorRT] VERBOSE: Tactic: 2 time 3.15833
[TensorRT] VERBOSE: Tactic: 5 time 4.25406
[TensorRT] VERBOSE: Tactic: 57 time 2.99141
[TensorRT] VERBOSE: Fastest Tactic: 0 Time: 2.97724
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5137655947464784826
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,112,12544,1204224) -> Float(1,56,3136,301056) ***************
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 9.4112
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 12.9144
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 19.5557
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 12.6694
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 10.1183
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 19.7525
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 9.23297
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 6.81224
[TensorRT] VERBOSE: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 19.2648
[TensorRT] VERBOSE: Fastest Tactic: -3456450830548107839 Time: 6.81224
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 5.78917
[TensorRT] VERBOSE: Tactic: 2 time 1.15401
[TensorRT] VERBOSE: Tactic: 5 time 457.529
[TensorRT] VERBOSE: Tactic: 57 time 5.79406
[TensorRT] VERBOSE: Fastest Tactic: 2 Time: 1.15401
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.86573
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.86573
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,56,3136,301056) -> Float(1,56,3136,75264) ***************
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.460313
[TensorRT] VERBOSE: Tactic: 655359 time 0.826927
[TensorRT] VERBOSE: Tactic: 786431 time 0.477292
[TensorRT] VERBOSE: Tactic: 851967 time 1.18917
[TensorRT] VERBOSE: Tactic: 1179647 time 0.619166
[TensorRT] VERBOSE: Tactic: 1310719 time 0.710782
[TensorRT] VERBOSE: Tactic: 1376255 time 0.434063
[TensorRT] VERBOSE: Tactic: 1441791 time 0.742448
[TensorRT] VERBOSE: Tactic: 1507327 time 1.08651
[TensorRT] VERBOSE: Tactic: 1638399 time 0.483021
[TensorRT] VERBOSE: Tactic: 1835007 time 0.452708
[TensorRT] VERBOSE: Tactic: 1900543 time 1.05938
[TensorRT] VERBOSE: Tactic: 2097151 time 0.506875
[TensorRT] VERBOSE: Tactic: 2162687 time 0.486719
[TensorRT] VERBOSE: Tactic: 2293759 time 0.429323
[TensorRT] VERBOSE: Tactic: 2359295 time 0.523437
[TensorRT] VERBOSE: Tactic: 2686975 time 0.453229
[TensorRT] VERBOSE: Tactic: 3080191 time 0.957187
[TensorRT] VERBOSE: Tactic: 3342335 time 1.11146
[TensorRT] VERBOSE: Tactic: 3407871 time 0.46599
[TensorRT] VERBOSE: Tactic: 3538943 time 0.489427
[TensorRT] VERBOSE: Tactic: 3670015 time 0.867343
[TensorRT] VERBOSE: Tactic: 3932159 time 0.870416
[TensorRT] VERBOSE: Tactic: 3997695 time 0.458542
[TensorRT] VERBOSE: Tactic: 4063231 time 1.08661
[TensorRT] VERBOSE: Tactic: 4194303 time 0.415573
[TensorRT] VERBOSE: Tactic: 4259839 time 0.479843
[TensorRT] VERBOSE: Tactic: 4325375 time 0.382448
[TensorRT] VERBOSE: Tactic: 4521983 time 0.408698
[TensorRT] VERBOSE: Tactic: 4587519 time 0.454844
[TensorRT] VERBOSE: Tactic: 4653055 time 0.752344
[TensorRT] VERBOSE: Tactic: 4915199 time 0.426771
[TensorRT] VERBOSE: Tactic: 4980735 time 0.42052
[TensorRT] VERBOSE: Tactic: 5177343 time 0.606146
[TensorRT] VERBOSE: Tactic: 5242879 time 0.476823
[TensorRT] VERBOSE: Tactic: 5373951 time 0.830469
[TensorRT] VERBOSE: Tactic: 5439487 time 0.71599
[TensorRT] VERBOSE: Tactic: 5570559 time 0.94875
[TensorRT] VERBOSE: Tactic: 5636095 time 1.08792
[TensorRT] VERBOSE: Tactic: 5701631 time 0.492552
[TensorRT] VERBOSE: Tactic: 5767167 time 1.43156
[TensorRT] VERBOSE: Tactic: 5832703 time 0.451615
[TensorRT] VERBOSE: Tactic: 5898239 time 0.51427
[TensorRT] VERBOSE: Tactic: 6029311 time 0.446042
[TensorRT] VERBOSE: Tactic: 6225919 time 0.496354
[TensorRT] VERBOSE: Tactic: 6291455 time 0.605208
[TensorRT] VERBOSE: Tactic: 6422527 time 0.84599
[TensorRT] VERBOSE: Tactic: 6750207 time 0.441667
[TensorRT] VERBOSE: Tactic: 6815743 time 0.683438
[TensorRT] VERBOSE: Tactic: 6946815 time 0.683021
[TensorRT] VERBOSE: Tactic: 7012351 time 0.502709
[TensorRT] VERBOSE: Tactic: 7077887 time 0.468646
[TensorRT] VERBOSE: Tactic: 7143423 time 0.574531
[TensorRT] VERBOSE: Tactic: 7208959 time 0.491667
[TensorRT] VERBOSE: Tactic: 7340031 time 0.510416
[TensorRT] VERBOSE: Tactic: 7405567 time 0.602396
[TensorRT] VERBOSE: Tactic: 7536639 time 0.629636
[TensorRT] VERBOSE: Tactic: 7602175 time 0.452968
[TensorRT] VERBOSE: Tactic: 7733247 time 0.532865
[TensorRT] VERBOSE: Tactic: 7798783 time 0.477135
[TensorRT] VERBOSE: Tactic: 8191999 time 0.552812
[TensorRT] VERBOSE: Tactic: 8257535 time 0.424792
[TensorRT] VERBOSE: Tactic: 8323071 time 0.445573
[TensorRT] VERBOSE: Tactic: 8650751 time 0.494792
[TensorRT] VERBOSE: Tactic: 8716287 time 0.701198
[TensorRT] VERBOSE: Tactic: 9109503 time 0.542135
[TensorRT] VERBOSE: Tactic: 9568255 time 0.421562
[TensorRT] VERBOSE: Tactic: 9895935 time 0.40849
[TensorRT] VERBOSE: Tactic: 10223615 time 0.431666
[TensorRT] VERBOSE: Tactic: 10354687 time 0.44323
[TensorRT] VERBOSE: Tactic: 10551295 time 0.379427
[TensorRT] VERBOSE: Tactic: 10747903 time 0.532188
[TensorRT] VERBOSE: Tactic: 10944511 time 0.421771
[TensorRT] VERBOSE: Fastest Tactic: 10551295 Time: 0.379427
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.194948
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.521302
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.267761
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.503125
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.276927
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.199062
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.207813
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.53276
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.28125
[TensorRT] VERBOSE: Fastest Tactic: 1062367460111450758 Time: 0.194948
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.31349
[TensorRT] VERBOSE: Tactic: 2 time 0.543854
[TensorRT] VERBOSE: Tactic: 5 time 1.0187
[TensorRT] VERBOSE: Tactic: 57 time 0.272864
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.272864
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 1062367460111450758
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,56,3136,75264) -> Float(1,56,3136,451584) ***************
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.425469
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.493646
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.348959
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.470521
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.366927
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.403178
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.412605
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.501667
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.371979
[TensorRT] VERBOSE: Fastest Tactic: 5137655947464784826 Time: 0.348959
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 1.04401
[TensorRT] VERBOSE: Tactic: 2 time 1.22422
[TensorRT] VERBOSE: Tactic: 5 time 1.64271
[TensorRT] VERBOSE: Tactic: 57 time 0.881406
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.881406
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5137655947464784826
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,56,3136,451584) -> Float(1,56,3136,451584) ***************
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 11.1408
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 15.5148
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 29.3926
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 14.8974
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 15.2608
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 29.7171
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 10.986
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 10.0556
[TensorRT] VERBOSE: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 28.8066
[TensorRT] VERBOSE: Fastest Tactic: -3456450830548107839 Time: 10.0556
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 1.57776
[TensorRT] VERBOSE: Tactic: 2 time 1.59214
[TensorRT] VERBOSE: Tactic: 5 time 174.57
[TensorRT] VERBOSE: Tactic: 6 time 20.9913
[TensorRT] VERBOSE: Tactic: 57 time 1.58661
[TensorRT] VERBOSE: Fastest Tactic: 0 Time: 1.57776
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.603021
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.603021
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,56,3136,451584), Float(1,56,3136,75264) -> Float(1,56,3136,75264) ***************
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.279167
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.755105
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.395312
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.733803
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.403854
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.269792
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.28401
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.743906
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.387448
[TensorRT] VERBOSE: Fastest Tactic: -6576203419454146580 Time: 0.269792
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.487969
[TensorRT] VERBOSE: Tactic: 2 time 0.868489
[TensorRT] VERBOSE: Tactic: 5 time 1.53974
[TensorRT] VERBOSE: Tactic: 57 time 0.43026
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.43026
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6576203419454146580
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,56,3136,75264) -> Float(1,56,3136,451584) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,56,3136,451584) -> Float(1,28,784,112896) ***************
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 5.24911
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 5.75859
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 8.97703
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 4.86156
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 5.42542
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 9.0651
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 5.40625
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 5.18641
[TensorRT] VERBOSE: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 8.82995
[TensorRT] VERBOSE: Fastest Tactic: 5137655947464784826 Time: 4.86156
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 3.07542
[TensorRT] VERBOSE: Tactic: 2 time 0.428698
[TensorRT] VERBOSE: Tactic: 5 time 173.017
[TensorRT] VERBOSE: Tactic: 57 time 3.75135
[TensorRT] VERBOSE: Fastest Tactic: 2 Time: 0.428698
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.363021
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.363021
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,112896) -> Float(1,28,784,25088) ***************
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.166042
[TensorRT] VERBOSE: Tactic: 655359 time 0.343646
[TensorRT] VERBOSE: Tactic: 786431 time 0.175104
[TensorRT] VERBOSE: Tactic: 851967 time 0.385364
[TensorRT] VERBOSE: Tactic: 1179647 time 0.227083
[TensorRT] VERBOSE: Tactic: 1310719 time 0.286197
[TensorRT] VERBOSE: Tactic: 1376255 time 0.202969
[TensorRT] VERBOSE: Tactic: 1441791 time 0.264844
[TensorRT] VERBOSE: Tactic: 1507327 time 0.391093
[TensorRT] VERBOSE: Tactic: 1638399 time 0.181771
[TensorRT] VERBOSE: Tactic: 1835007 time 0.154636
[TensorRT] VERBOSE: Tactic: 1900543 time 0.358021
[TensorRT] VERBOSE: Tactic: 2162687 time 0.192605
[TensorRT] VERBOSE: Tactic: 2293759 time 0.195573
[TensorRT] VERBOSE: Tactic: 2359295 time 0.218229
[TensorRT] VERBOSE: Tactic: 2686975 time 0.192865
[TensorRT] VERBOSE: Tactic: 3080191 time 0.350729
[TensorRT] VERBOSE: Tactic: 3342335 time 0.377188
[TensorRT] VERBOSE: Tactic: 3407871 time 0.196302
[TensorRT] VERBOSE: Tactic: 3538943 time 0.196145
[TensorRT] VERBOSE: Tactic: 3670015 time 0.370208
[TensorRT] VERBOSE: Tactic: 3932159 time 0.344427
[TensorRT] VERBOSE: Tactic: 3997695 time 0.176198
[TensorRT] VERBOSE: Tactic: 4063231 time 0.380052
[TensorRT] VERBOSE: Tactic: 4194303 time 0.145937
[TensorRT] VERBOSE: Tactic: 4325375 time 0.156666
[TensorRT] VERBOSE: Tactic: 4521983 time 0.192188
[TensorRT] VERBOSE: Tactic: 4587519 time 0.179167
[TensorRT] VERBOSE: Tactic: 4653055 time 0.27948
[TensorRT] VERBOSE: Tactic: 4915199 time 0.147448
[TensorRT] VERBOSE: Tactic: 4980735 time 0.164739
[TensorRT] VERBOSE: Tactic: 5177343 time 0.221302
[TensorRT] VERBOSE: Tactic: 5242879 time 0.186458
[TensorRT] VERBOSE: Tactic: 5373951 time 0.308489
[TensorRT] VERBOSE: Tactic: 5439487 time 0.210052
[TensorRT] VERBOSE: Tactic: 5570559 time 0.341354
[TensorRT] VERBOSE: Tactic: 5636095 time 0.375573
[TensorRT] VERBOSE: Tactic: 5701631 time 0.221094
[TensorRT] VERBOSE: Tactic: 5767167 time 0.342864
[TensorRT] VERBOSE: Tactic: 5832703 time 0.193334
[TensorRT] VERBOSE: Tactic: 5898239 time 0.176354
[TensorRT] VERBOSE: Tactic: 6029311 time 0.179479
[TensorRT] VERBOSE: Tactic: 6225919 time 0.174271
[TensorRT] VERBOSE: Tactic: 6291455 time 0.227135
[TensorRT] VERBOSE: Tactic: 6422527 time 0.303854
[TensorRT] VERBOSE: Tactic: 6750207 time 0.161354
[TensorRT] VERBOSE: Tactic: 6815743 time 0.223177
[TensorRT] VERBOSE: Tactic: 6946815 time 0.228333
[TensorRT] VERBOSE: Tactic: 7077887 time 0.189948
[TensorRT] VERBOSE: Tactic: 7143423 time 0.220365
[TensorRT] VERBOSE: Tactic: 7208959 time 0.187187
[TensorRT] VERBOSE: Tactic: 7340031 time 0.177969
[TensorRT] VERBOSE: Tactic: 7405567 time 0.218542
[TensorRT] VERBOSE: Tactic: 7536639 time 0.238698
[TensorRT] VERBOSE: Tactic: 7602175 time 0.201823
[TensorRT] VERBOSE: Tactic: 7733247 time 0.191875
[TensorRT] VERBOSE: Tactic: 7798783 time 0.168594
[TensorRT] VERBOSE: Tactic: 8191999 time 0.215
[TensorRT] VERBOSE: Tactic: 8257535 time 0.149688
[TensorRT] VERBOSE: Tactic: 8323071 time 0.160625
[TensorRT] VERBOSE: Tactic: 8650751 time 0.19427
[TensorRT] VERBOSE: Tactic: 8716287 time 0.23073
[TensorRT] VERBOSE: Tactic: 9568255 time 0.1475
[TensorRT] VERBOSE: Tactic: 9895935 time 0.155729
[TensorRT] VERBOSE: Tactic: 10223615 time 0.192917
[TensorRT] VERBOSE: Tactic: 10354687 time 0.172917
[TensorRT] VERBOSE: Tactic: 10551295 time 0.145
[TensorRT] VERBOSE: Tactic: 10747903 time 0.18526
[TensorRT] VERBOSE: Tactic: 10944511 time 0.159636
[TensorRT] VERBOSE: Fastest Tactic: 10551295 Time: 0.145
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.094323
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.20974
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.114844
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.215261
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.130833
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.104063
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.09026
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.205782
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.110781
[TensorRT] VERBOSE: Fastest Tactic: -3456450830548107839 Time: 0.09026
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.129271
[TensorRT] VERBOSE: Tactic: 2 time 0.233958
[TensorRT] VERBOSE: Tactic: 5 time 0.569739
[TensorRT] VERBOSE: Tactic: 57 time 0.130208
[TensorRT] VERBOSE: Fastest Tactic: 0 Time: 0.129271
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -3456450830548107839
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,25088) -> Float(1,28,784,150528) ***************
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.152969
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.154844
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.121615
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.156406
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.124219
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.164635
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.16625
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.175365
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.136719
[TensorRT] VERBOSE: Fastest Tactic: 5137655947464784826 Time: 0.121615
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.386146
[TensorRT] VERBOSE: Tactic: 2 time 0.442552
[TensorRT] VERBOSE: Tactic: 5 time 0.935312
[TensorRT] VERBOSE: Tactic: 57 time 0.329583
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.329583
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 5137655947464784826
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,150528) -> Float(1,28,784,150528) ***************
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 7.15818
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 6.55568
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 11.9927
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 6.24745
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 6.45932
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 12.1646
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 5.74937
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 5.53438
[TensorRT] VERBOSE: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 11.753
[TensorRT] VERBOSE: Fastest Tactic: -3456450830548107839 Time: 5.53438
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 4.60474
[TensorRT] VERBOSE: Tactic: 2 time 0.548646
[TensorRT] VERBOSE: Tactic: 5 time 62.0208
[TensorRT] VERBOSE: Tactic: 6 time 9.50411
[TensorRT] VERBOSE: Tactic: 57 time 4.74703
[TensorRT] VERBOSE: Fastest Tactic: 2 Time: 0.548646
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.255677
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.255677
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,150528), Float(1,28,784,25088) -> Float(1,28,784,25088) ***************
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.140886
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.286771
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.156927
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.266146
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.146771
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.111667
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.134739
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.28427
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.157917
[TensorRT] VERBOSE: Fastest Tactic: -6576203419454146580 Time: 0.111667
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.190104
[TensorRT] VERBOSE: Tactic: 2 time 0.34698
[TensorRT] VERBOSE: Tactic: 5 time 0.761823
[TensorRT] VERBOSE: Tactic: 57 time 0.14901
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.14901
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6576203419454146580
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,25088) -> Float(1,28,784,150528) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,150528) -> Float(1,28,784,150528) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,150528), Float(1,28,784,25088) -> Float(1,28,784,25088) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,25088) -> Float(1,28,784,150528) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,28,784,150528) -> Float(1,14,196,37632) ***************
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 5.1099
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 3.27531
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 5.08724
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 4.91328
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 2.79562
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 5.17193
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 5.34229
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 4.75089
[TensorRT] VERBOSE: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 5.07464
[TensorRT] VERBOSE: Fastest Tactic: 6645123197870846056 Time: 2.79562
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.182396
[TensorRT] VERBOSE: Tactic: 2 time 0.190834
[TensorRT] VERBOSE: Tactic: 5 time 61.8431
[TensorRT] VERBOSE: Tactic: 57 time 0.18177
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.18177
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.158386
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.158386
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,37632) -> Float(1,14,196,12544) ***************
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.147656
[TensorRT] VERBOSE: Tactic: 655359 time 0.14
[TensorRT] VERBOSE: Tactic: 786431 time 0.098802
[TensorRT] VERBOSE: Tactic: 851967 time 0.127917
[TensorRT] VERBOSE: Tactic: 1179647 time 0.085938
[TensorRT] VERBOSE: Tactic: 1310719 time 0.219895
[TensorRT] VERBOSE: Tactic: 1376255 time 0.113594
[TensorRT] VERBOSE: Tactic: 1441791 time 0.120834
[TensorRT] VERBOSE: Tactic: 1507327 time 0.188438
[TensorRT] VERBOSE: Tactic: 1638399 time 0.133906
[TensorRT] VERBOSE: Tactic: 1835007 time 0.10474
[TensorRT] VERBOSE: Tactic: 1900543 time 0.186093
[TensorRT] VERBOSE: Tactic: 2097151 time 0.117135
[TensorRT] VERBOSE: Tactic: 2162687 time 0.105209
[TensorRT] VERBOSE: Tactic: 2293759 time 0.104271
[TensorRT] VERBOSE: Tactic: 2359295 time 0.130052
[TensorRT] VERBOSE: Tactic: 2686975 time 0.114948
[TensorRT] VERBOSE: Tactic: 3080191 time 0.136041
[TensorRT] VERBOSE: Tactic: 3342335 time 0.165052
[TensorRT] VERBOSE: Tactic: 3407871 time 0.086822
[TensorRT] VERBOSE: Tactic: 3538943 time 0.079167
[TensorRT] VERBOSE: Tactic: 3670015 time 0.163697
[TensorRT] VERBOSE: Tactic: 3932159 time 0.149062
[TensorRT] VERBOSE: Tactic: 3997695 time 0.099219
[TensorRT] VERBOSE: Tactic: 4063231 time 0.137448
[TensorRT] VERBOSE: Tactic: 4194303 time 0.096406
[TensorRT] VERBOSE: Tactic: 4259839 time 0.116354
[TensorRT] VERBOSE: Tactic: 4325375 time 0.117187
[TensorRT] VERBOSE: Tactic: 4521983 time 0.117448
[TensorRT] VERBOSE: Tactic: 4587519 time 0.109115
[TensorRT] VERBOSE: Tactic: 4653055 time 0.094583
[TensorRT] VERBOSE: Tactic: 4915199 time 0.098906
[TensorRT] VERBOSE: Tactic: 4980735 time 0.132239
[TensorRT] VERBOSE: Tactic: 5177343 time 0.092031
[TensorRT] VERBOSE: Tactic: 5242879 time 0.080416
[TensorRT] VERBOSE: Tactic: 5373951 time 0.096406
[TensorRT] VERBOSE: Tactic: 5439487 time 0.099166
[TensorRT] VERBOSE: Tactic: 5570559 time 0.125886
[TensorRT] VERBOSE: Tactic: 5636095 time 0.117448
[TensorRT] VERBOSE: Tactic: 5701631 time 0.10151
[TensorRT] VERBOSE: Tactic: 5767167 time 0.122656
[TensorRT] VERBOSE: Tactic: 5832703 time 0.08125
[TensorRT] VERBOSE: Tactic: 5898239 time 0.072761
[TensorRT] VERBOSE: Tactic: 6029311 time 0.098542
[TensorRT] VERBOSE: Tactic: 6225919 time 0.073437
[TensorRT] VERBOSE: Tactic: 6291455 time 0.084219
[TensorRT] VERBOSE: Tactic: 6422527 time 0.106771
[TensorRT] VERBOSE: Tactic: 6750207 time 0.096927
[TensorRT] VERBOSE: Tactic: 6815743 time 0.077396
[TensorRT] VERBOSE: Tactic: 6946815 time 0.126146
[TensorRT] VERBOSE: Tactic: 7012351 time 0.106823
[TensorRT] VERBOSE: Tactic: 7077887 time 0.076146
[TensorRT] VERBOSE: Tactic: 7143423 time 0.128177
[TensorRT] VERBOSE: Tactic: 7208959 time 0.094063
[TensorRT] VERBOSE: Tactic: 7340031 time 0.082136
[TensorRT] VERBOSE: Tactic: 7405567 time 0.093646
[TensorRT] VERBOSE: Tactic: 7536639 time 0.105104
[TensorRT] VERBOSE: Tactic: 7602175 time 0.121094
[TensorRT] VERBOSE: Tactic: 7733247 time 0.078073
[TensorRT] VERBOSE: Tactic: 7798783 time 0.099063
[TensorRT] VERBOSE: Tactic: 8191999 time 0.132865
[TensorRT] VERBOSE: Tactic: 8257535 time 0.100365
[TensorRT] VERBOSE: Tactic: 8323071 time 0.092708
[TensorRT] VERBOSE: Tactic: 8650751 time 0.127448
[TensorRT] VERBOSE: Tactic: 8716287 time 0.074427
[TensorRT] VERBOSE: Tactic: 9109503 time 0.116354
[TensorRT] VERBOSE: Tactic: 9568255 time 0.107031
[TensorRT] VERBOSE: Tactic: 9895935 time 0.096406
[TensorRT] VERBOSE: Tactic: 10223615 time 0.105313
[TensorRT] VERBOSE: Tactic: 10354687 time 0.11125
[TensorRT] VERBOSE: Tactic: 10551295 time 0.086771
[TensorRT] VERBOSE: Tactic: 10747903 time 0.076042
[TensorRT] VERBOSE: Tactic: 10944511 time 0.130052
[TensorRT] VERBOSE: Fastest Tactic: 5898239 Time: 0.072761
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.063333
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.096302
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.071406
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.092708
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.053021
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.057864
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.053021
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.079166
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.048541
[TensorRT] VERBOSE: Fastest Tactic: -37215280111360163 Time: 0.048541
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.098333
[TensorRT] VERBOSE: Tactic: 2 time 0.236563
[TensorRT] VERBOSE: Tactic: 5 time 0.876979
[TensorRT] VERBOSE: Tactic: 57 time 0.108281
[TensorRT] VERBOSE: Fastest Tactic: 0 Time: 0.098333
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -37215280111360163
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,12544) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.312343
[TensorRT] VERBOSE: Tactic: 655359 time 0.27151
[TensorRT] VERBOSE: Tactic: 786431 time 0.311406
[TensorRT] VERBOSE: Tactic: 851967 time 0.309428
[TensorRT] VERBOSE: Tactic: 1179647 time 0.293802
[TensorRT] VERBOSE: Tactic: 1310719 time 0.520468
[TensorRT] VERBOSE: Tactic: 1376255 time 0.255573
[TensorRT] VERBOSE: Tactic: 1441791 time 0.375572
[TensorRT] VERBOSE: Tactic: 1507327 time 0.326458
[TensorRT] VERBOSE: Tactic: 1638399 time 0.392292
[TensorRT] VERBOSE: Tactic: 1835007 time 0.314688
[TensorRT] VERBOSE: Tactic: 1900543 time 0.315989
[TensorRT] VERBOSE: Tactic: 2097151 time 0.365677
[TensorRT] VERBOSE: Tactic: 2162687 time 0.264688
[TensorRT] VERBOSE: Tactic: 2293759 time 0.244739
[TensorRT] VERBOSE: Tactic: 2359295 time 0.311042
[TensorRT] VERBOSE: Tactic: 2686975 time 0.244011
[TensorRT] VERBOSE: Tactic: 3080191 time 0.261927
[TensorRT] VERBOSE: Tactic: 3342335 time 0.314323
[TensorRT] VERBOSE: Tactic: 3407871 time 0.266302
[TensorRT] VERBOSE: Tactic: 3538943 time 0.254062
[TensorRT] VERBOSE: Tactic: 3670015 time 0.267031
[TensorRT] VERBOSE: Tactic: 3932159 time 0.270781
[TensorRT] VERBOSE: Tactic: 3997695 time 0.304636
[TensorRT] VERBOSE: Tactic: 4063231 time 0.284635
[TensorRT] VERBOSE: Tactic: 4194303 time 0.313386
[TensorRT] VERBOSE: Tactic: 4259839 time 0.380417
[TensorRT] VERBOSE: Tactic: 4325375 time 0.341094
[TensorRT] VERBOSE: Tactic: 4521983 time 0.29875
[TensorRT] VERBOSE: Tactic: 4587519 time 0.342604
[TensorRT] VERBOSE: Tactic: 4653055 time 0.336667
[TensorRT] VERBOSE: Tactic: 4915199 time 0.316615
[TensorRT] VERBOSE: Tactic: 4980735 time 0.342291
[TensorRT] VERBOSE: Tactic: 5177343 time 0.314947
[TensorRT] VERBOSE: Tactic: 5242879 time 0.246719
[TensorRT] VERBOSE: Tactic: 5373951 time 0.339531
[TensorRT] VERBOSE: Tactic: 5439487 time 0.31526
[TensorRT] VERBOSE: Tactic: 5570559 time 0.278646
[TensorRT] VERBOSE: Tactic: 5636095 time 0.274791
[TensorRT] VERBOSE: Tactic: 5701631 time 0.254948
[TensorRT] VERBOSE: Tactic: 5767167 time 0.36474
[TensorRT] VERBOSE: Tactic: 5832703 time 0.254427
[TensorRT] VERBOSE: Tactic: 5898239 time 0.259323
[TensorRT] VERBOSE: Tactic: 6029311 time 0.23276
[TensorRT] VERBOSE: Tactic: 6225919 time 0.24198
[TensorRT] VERBOSE: Tactic: 6291455 time 0.29776
[TensorRT] VERBOSE: Tactic: 6422527 time 0.244063
[TensorRT] VERBOSE: Tactic: 6750207 time 0.295469
[TensorRT] VERBOSE: Tactic: 6815743 time 0.270052
[TensorRT] VERBOSE: Tactic: 6946815 time 0.35474
[TensorRT] VERBOSE: Tactic: 7012351 time 0.363021
[TensorRT] VERBOSE: Tactic: 7077887 time 0.251875
[TensorRT] VERBOSE: Tactic: 7143423 time 0.352344
[TensorRT] VERBOSE: Tactic: 7208959 time 0.276771
[TensorRT] VERBOSE: Tactic: 7340031 time 0.267188
[TensorRT] VERBOSE: Tactic: 7405567 time 0.270573
[TensorRT] VERBOSE: Tactic: 7536639 time 0.279687
[TensorRT] VERBOSE: Tactic: 7602175 time 0.330625
[TensorRT] VERBOSE: Tactic: 7733247 time 0.26677
[TensorRT] VERBOSE: Tactic: 7798783 time 0.310729
[TensorRT] VERBOSE: Tactic: 8191999 time 0.372553
[TensorRT] VERBOSE: Tactic: 8257535 time 0.304948
[TensorRT] VERBOSE: Tactic: 8323071 time 0.295052
[TensorRT] VERBOSE: Tactic: 8650751 time 0.35625
[TensorRT] VERBOSE: Tactic: 8716287 time 0.279375
[TensorRT] VERBOSE: Tactic: 9109503 time 0.386823
[TensorRT] VERBOSE: Tactic: 9568255 time 0.317136
[TensorRT] VERBOSE: Tactic: 9895935 time 0.312083
[TensorRT] VERBOSE: Tactic: 10223615 time 0.243073
[TensorRT] VERBOSE: Tactic: 10354687 time 0.356875
[TensorRT] VERBOSE: Tactic: 10551295 time 0.270469
[TensorRT] VERBOSE: Tactic: 10747903 time 0.260782
[TensorRT] VERBOSE: Tactic: 10944511 time 0.34151
[TensorRT] VERBOSE: Fastest Tactic: 6029311 Time: 0.23276
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.134322
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.104896
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.114896
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.116927
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.117813
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.138229
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.143229
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.102291
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.099896
[TensorRT] VERBOSE: Fastest Tactic: -37215280111360163 Time: 0.099896
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.328802
[TensorRT] VERBOSE: Tactic: 2 time 0.368542
[TensorRT] VERBOSE: Tactic: 5 time 2.63469
[TensorRT] VERBOSE: Tactic: 57 time 0.299636
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.299636
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -37215280111360163
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 12.8476
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 13.4589
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 13.0799
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 12.6167
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 12.6954
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 13.409
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 13.4789
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 11.8624
[TensorRT] VERBOSE: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 12.5874
[TensorRT] VERBOSE: Fastest Tactic: -3456450830548107839 Time: 11.8624
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.326511
[TensorRT] VERBOSE: Tactic: 2 time 0.322553
[TensorRT] VERBOSE: Tactic: 5 time 123.906
[TensorRT] VERBOSE: Tactic: 6 time 19.6532
[TensorRT] VERBOSE: Tactic: 57 time 0.324375
[TensorRT] VERBOSE: Fastest Tactic: 2 Time: 0.322553
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.21776
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.21776
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264), Float(1,14,196,12544) -> Float(1,14,196,12544) ***************
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.110833
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.145469
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.109479
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.149375
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.116563
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.115
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.119219
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.15349
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.107604
[TensorRT] VERBOSE: Fastest Tactic: -37215280111360163 Time: 0.107604
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.201979
[TensorRT] VERBOSE: Tactic: 2 time 0.37849
[TensorRT] VERBOSE: Tactic: 5 time 1.76255
[TensorRT] VERBOSE: Tactic: 57 time 0.174688
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.174688
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -37215280111360163
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,12544) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264), Float(1,14,196,12544) -> Float(1,14,196,12544) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,12544) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264), Float(1,14,196,12544) -> Float(1,14,196,12544) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,12544) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264) -> Float(1,14,196,75264) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,75264) -> Float(1,14,196,18816) ***************
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.301146
[TensorRT] VERBOSE: Tactic: 655359 time 0.322604
[TensorRT] VERBOSE: Tactic: 786431 time 0.250052
[TensorRT] VERBOSE: Tactic: 851967 time 0.313281
[TensorRT] VERBOSE: Tactic: 1179647 time 0.300625
[TensorRT] VERBOSE: Tactic: 1310719 time 0.5525
[TensorRT] VERBOSE: Tactic: 1376255 time 0.229114
[TensorRT] VERBOSE: Tactic: 1441791 time 0.385989
[TensorRT] VERBOSE: Tactic: 1507327 time 0.385781
[TensorRT] VERBOSE: Tactic: 1638399 time 0.361041
[TensorRT] VERBOSE: Tactic: 1835007 time 0.256197
[TensorRT] VERBOSE: Tactic: 1900543 time 0.361927
[TensorRT] VERBOSE: Tactic: 2097151 time 0.273542
[TensorRT] VERBOSE: Tactic: 2162687 time 0.228594
[TensorRT] VERBOSE: Tactic: 2293759 time 0.242448
[TensorRT] VERBOSE: Tactic: 2359295 time 0.348125
[TensorRT] VERBOSE: Tactic: 2686975 time 0.347188
[TensorRT] VERBOSE: Tactic: 3080191 time 0.265
[TensorRT] VERBOSE: Tactic: 3342335 time 0.294479
[TensorRT] VERBOSE: Tactic: 3407871 time 0.243593
[TensorRT] VERBOSE: Tactic: 3538943 time 0.255781
[TensorRT] VERBOSE: Tactic: 3670015 time 0.342344
[TensorRT] VERBOSE: Tactic: 3932159 time 0.317395
[TensorRT] VERBOSE: Tactic: 3997695 time 0.253334
[TensorRT] VERBOSE: Tactic: 4063231 time 0.251458
[TensorRT] VERBOSE: Tactic: 4194303 time 0.247031
[TensorRT] VERBOSE: Tactic: 4259839 time 0.29427
[TensorRT] VERBOSE: Tactic: 4325375 time 0.301614
[TensorRT] VERBOSE: Tactic: 4521983 time 0.396198
[TensorRT] VERBOSE: Tactic: 4587519 time 0.271667
[TensorRT] VERBOSE: Tactic: 4653055 time 0.317604
[TensorRT] VERBOSE: Tactic: 4915199 time 0.228021
[TensorRT] VERBOSE: Tactic: 4980735 time 0.297135
[TensorRT] VERBOSE: Tactic: 5177343 time 0.362031
[TensorRT] VERBOSE: Tactic: 5242879 time 0.218229
[TensorRT] VERBOSE: Tactic: 5373951 time 0.338125
[TensorRT] VERBOSE: Tactic: 5439487 time 0.251406
[TensorRT] VERBOSE: Tactic: 5570559 time 0.280573
[TensorRT] VERBOSE: Tactic: 5636095 time 0.25099
[TensorRT] VERBOSE: Tactic: 5701631 time 0.329739
[TensorRT] VERBOSE: Tactic: 5767167 time 0.345625
[TensorRT] VERBOSE: Tactic: 5832703 time 0.236354
[TensorRT] VERBOSE: Tactic: 5898239 time 0.23823
[TensorRT] VERBOSE: Tactic: 6029311 time 0.214114
[TensorRT] VERBOSE: Tactic: 6225919 time 0.212813
[TensorRT] VERBOSE: Tactic: 6291455 time 0.28849
[TensorRT] VERBOSE: Tactic: 6422527 time 0.261562
[TensorRT] VERBOSE: Tactic: 6750207 time 0.234844
[TensorRT] VERBOSE: Tactic: 6815743 time 0.224948
[TensorRT] VERBOSE: Tactic: 6946815 time 0.333906
[TensorRT] VERBOSE: Tactic: 7012351 time 0.274896
[TensorRT] VERBOSE: Tactic: 7077887 time 0.238125
[TensorRT] VERBOSE: Tactic: 7143423 time 0.359375
[TensorRT] VERBOSE: Tactic: 7208959 time 0.289531
[TensorRT] VERBOSE: Tactic: 7340031 time 0.259115
[TensorRT] VERBOSE: Tactic: 7405567 time 0.272812
[TensorRT] VERBOSE: Tactic: 7536639 time 0.305156
[TensorRT] VERBOSE: Tactic: 7602175 time 0.314375
[TensorRT] VERBOSE: Tactic: 7733247 time 0.245781
[TensorRT] VERBOSE: Tactic: 7798783 time 0.25698
[TensorRT] VERBOSE: Tactic: 8191999 time 0.37901
[TensorRT] VERBOSE: Tactic: 8257535 time 0.240104
[TensorRT] VERBOSE: Tactic: 8323071 time 0.24
[TensorRT] VERBOSE: Tactic: 8650751 time 0.33151
[TensorRT] VERBOSE: Tactic: 8716287 time 0.236615
[TensorRT] VERBOSE: Tactic: 9109503 time 0.311302
[TensorRT] VERBOSE: Tactic: 9568255 time 0.242344
[TensorRT] VERBOSE: Tactic: 9895935 time 0.256771
[TensorRT] VERBOSE: Tactic: 10223615 time 0.348333
[TensorRT] VERBOSE: Tactic: 10354687 time 0.27099
[TensorRT] VERBOSE: Tactic: 10551295 time 0.234062
[TensorRT] VERBOSE: Tactic: 10747903 time 0.230521
[TensorRT] VERBOSE: Tactic: 10944511 time 0.294687
[TensorRT] VERBOSE: Fastest Tactic: 6225919 Time: 0.212813
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.166146
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.155104
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.154583
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.139219
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.154063
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.124427
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.144427
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.142917
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.15625
[TensorRT] VERBOSE: Fastest Tactic: -6576203419454146580 Time: 0.124427
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.267552
[TensorRT] VERBOSE: Tactic: 2 time 0.419167
[TensorRT] VERBOSE: Tactic: 5 time 2.67635
[TensorRT] VERBOSE: Tactic: 57 time 0.215989
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.215989
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6576203419454146580
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,18816) -> Float(1,14,196,112896) ***************
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.270625
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.232969
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.210625
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.214166
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.199584
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.223177
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.253438
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.229427
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.205782
[TensorRT] VERBOSE: Fastest Tactic: 6645123197870846056 Time: 0.199584
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.543073
[TensorRT] VERBOSE: Tactic: 2 time 0.566094
[TensorRT] VERBOSE: Tactic: 5 time 5.74787
[TensorRT] VERBOSE: Tactic: 57 time 0.49302
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.49302
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: 6645123197870846056
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,112896) -> Float(1,14,196,112896) ***************
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 15.2343
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 16.539
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 15.8776
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 14.8949
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 15.1886
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 16.4586
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 16.451
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 14.9914
[TensorRT] VERBOSE: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 14.8865
[TensorRT] VERBOSE: Fastest Tactic: -410470605513481746 Time: 14.8865
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.473438
[TensorRT] VERBOSE: Tactic: 2 time 0.46875
[TensorRT] VERBOSE: Tactic: 5 time 185.222
[TensorRT] VERBOSE: Tactic: 6 time 52.6171
[TensorRT] VERBOSE: Tactic: 57 time 0.47
[TensorRT] VERBOSE: Fastest Tactic: 2 Time: 0.46875
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.323541
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.323541
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaDepthwiseConvolution Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,112896), Float(1,14,196,18816) -> Float(1,14,196,18816) ***************
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.194271
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.209167
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.232917
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.212605
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.236302
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.192239
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.202135
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.214323
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.22875
[TensorRT] VERBOSE: Fastest Tactic: -6576203419454146580 Time: 0.192239
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.415
[TensorRT] VERBOSE: Tactic: 2 time 0.529948
[TensorRT] VERBOSE: Tactic: 5 time 3.83078
[TensorRT] VERBOSE: Tactic: 57 time 0.346094
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.346094
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -6576203419454146580
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,18816) -> Float(1,14,196,112896) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,112896) -> Float(1,14,196,112896) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,112896), Float(1,14,196,18816) -> Float(1,14,196,18816) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,18816) -> Float(1,14,196,112896) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,14,196,112896) -> Float(1,7,49,28224) ***************
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 15.349
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 16.2765
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 15.5988
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 15.3095
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 15.5681
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 16.4873
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 16.4183
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 14.983
[TensorRT] VERBOSE: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 14.9239
[TensorRT] VERBOSE: Fastest Tactic: -410470605513481746 Time: 14.9239
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.22526
[TensorRT] VERBOSE: Tactic: 2 time 0.221875
[TensorRT] VERBOSE: Tactic: 5 time 183.714
[TensorRT] VERBOSE: Tactic: 57 time 0.22125
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.22125
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.238177
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.238177
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaConvolution Tactic: 57
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,28224) -> Float(1,7,49,7840) ***************
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.391823
[TensorRT] VERBOSE: Tactic: 655359 time 0.241354
[TensorRT] VERBOSE: Tactic: 786431 time 0.232708
[TensorRT] VERBOSE: Tactic: 851967 time 0.29
[TensorRT] VERBOSE: Tactic: 1179647 time 0.193698
[TensorRT] VERBOSE: Tactic: 1310719 time 0.394114
[TensorRT] VERBOSE: Tactic: 1376255 time 0.258594
[TensorRT] VERBOSE: Tactic: 1441791 time 0.295625
[TensorRT] VERBOSE: Tactic: 1507327 time 0.27901
[TensorRT] VERBOSE: Tactic: 1638399 time 0.256822
[TensorRT] VERBOSE: Tactic: 1835007 time 0.215729
[TensorRT] VERBOSE: Tactic: 1900543 time 0.484635
[TensorRT] VERBOSE: Tactic: 2097151 time 0.166093
[TensorRT] VERBOSE: Tactic: 2162687 time 0.259375
[TensorRT] VERBOSE: Tactic: 2293759 time 0.238854
[TensorRT] VERBOSE: Tactic: 2359295 time 0.286666
[TensorRT] VERBOSE: Tactic: 2686975 time 0.251823
[TensorRT] VERBOSE: Tactic: 3080191 time 0.260886
[TensorRT] VERBOSE: Tactic: 3342335 time 0.401146
[TensorRT] VERBOSE: Tactic: 3407871 time 0.158177
[TensorRT] VERBOSE: Tactic: 3538943 time 0.163541
[TensorRT] VERBOSE: Tactic: 3670015 time 0.487396
[TensorRT] VERBOSE: Tactic: 3932159 time 0.216458
[TensorRT] VERBOSE: Tactic: 3997695 time 0.242396
[TensorRT] VERBOSE: Tactic: 4063231 time 0.212813
[TensorRT] VERBOSE: Tactic: 4194303 time 0.135521
[TensorRT] VERBOSE: Tactic: 4259839 time 0.177552
[TensorRT] VERBOSE: Tactic: 4325375 time 0.218542
[TensorRT] VERBOSE: Tactic: 4521983 time 0.249636
[TensorRT] VERBOSE: Tactic: 4587519 time 0.194115
[TensorRT] VERBOSE: Tactic: 4653055 time 0.222292
[TensorRT] VERBOSE: Tactic: 4915199 time 0.137291
[TensorRT] VERBOSE: Tactic: 4980735 time 0.216198
[TensorRT] VERBOSE: Tactic: 5177343 time 0.205781
[TensorRT] VERBOSE: Tactic: 5242879 time 0.146615
[TensorRT] VERBOSE: Tactic: 5373951 time 0.200261
[TensorRT] VERBOSE: Tactic: 5439487 time 0.198385
[TensorRT] VERBOSE: Tactic: 5570559 time 0.252396
[TensorRT] VERBOSE: Tactic: 5636095 time 0.224427
[TensorRT] VERBOSE: Tactic: 5701631 time 0.246459
[TensorRT] VERBOSE: Tactic: 5767167 time 0.28224
[TensorRT] VERBOSE: Tactic: 5832703 time 0.153593
[TensorRT] VERBOSE: Tactic: 5898239 time 0.130573
[TensorRT] VERBOSE: Tactic: 6029311 time 0.227708
[TensorRT] VERBOSE: Tactic: 6225919 time 0.150677
[TensorRT] VERBOSE: Tactic: 6291455 time 0.204688
[TensorRT] VERBOSE: Tactic: 6422527 time 0.233125
[TensorRT] VERBOSE: Tactic: 6750207 time 0.172916
[TensorRT] VERBOSE: Tactic: 6815743 time 0.1425
[TensorRT] VERBOSE: Tactic: 6946815 time 0.223177
[TensorRT] VERBOSE: Tactic: 7012351 time 0.163698
[TensorRT] VERBOSE: Tactic: 7077887 time 0.159844
[TensorRT] VERBOSE: Tactic: 7143423 time 0.297448
[TensorRT] VERBOSE: Tactic: 7208959 time 0.155886
[TensorRT] VERBOSE: Tactic: 7340031 time 0.13177
[TensorRT] VERBOSE: Tactic: 7405567 time 0.218282
[TensorRT] VERBOSE: Tactic: 7536639 time 0.222292
[TensorRT] VERBOSE: Tactic: 7602175 time 0.22349
[TensorRT] VERBOSE: Tactic: 7733247 time 0.156093
[TensorRT] VERBOSE: Tactic: 7798783 time 0.234584
[TensorRT] VERBOSE: Tactic: 8191999 time 0.211094
[TensorRT] VERBOSE: Tactic: 8257535 time 0.136979
[TensorRT] VERBOSE: Tactic: 8323071 time 0.204427
[TensorRT] VERBOSE: Tactic: 8650751 time 0.223802
[TensorRT] VERBOSE: Tactic: 8716287 time 0.152865
[TensorRT] VERBOSE: Tactic: 9109503 time 0.159479
[TensorRT] VERBOSE: Tactic: 9568255 time 0.134688
[TensorRT] VERBOSE: Tactic: 9895935 time 0.136562
[TensorRT] VERBOSE: Tactic: 10223615 time 0.239687
[TensorRT] VERBOSE: Tactic: 10354687 time 0.191875
[TensorRT] VERBOSE: Tactic: 10551295 time 0.160729
[TensorRT] VERBOSE: Tactic: 10747903 time 0.15599
[TensorRT] VERBOSE: Tactic: 10944511 time 0.210313
[TensorRT] VERBOSE: Fastest Tactic: 5898239 Time: 0.130573
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.209219
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.208594
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.180052
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.209479
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.175521
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.180521
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.185469
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.202812
[TensorRT] VERBOSE: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.175573
[TensorRT] VERBOSE: Fastest Tactic: 6645123197870846056 Time: 0.175521
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.218906
[TensorRT] VERBOSE: Tactic: 2 time 0.368073
[TensorRT] VERBOSE: Tactic: 5 time 7.4438
[TensorRT] VERBOSE: Tactic: 57 time 0.187709
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.187709
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 5898239
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,7840) -> Float(1,7,49,47040) ***************
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.313021
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.275834
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.262344
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.266979
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.253021
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.279739
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.314166
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.272709
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.24474
[TensorRT] VERBOSE: Fastest Tactic: -37215280111360163 Time: 0.24474
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.465469
[TensorRT] VERBOSE: Tactic: 2 time 0.534167
[TensorRT] VERBOSE: Tactic: 5 time 14.9475
[TensorRT] VERBOSE: Tactic: 57 time 0.423177
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.423177
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -37215280111360163
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,47040) -> Float(1,7,49,47040) ***************
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 25.847
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: 4337000649858996379 time 27.5123
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 26.938
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 25.2349
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 24.2166
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -9137461792520977713 time 27.6497
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_large_nn_v1
[TensorRT] VERBOSE: Tactic: -6092040395344634144 time 27.8739
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 25.444
[TensorRT] VERBOSE: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 24.9521
[TensorRT] VERBOSE: Fastest Tactic: 6645123197870846056 Time: 24.2166
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.324531
[TensorRT] VERBOSE: Tactic: 2 time 0.324739
[TensorRT] VERBOSE: Tactic: 5 time 307.971
[TensorRT] VERBOSE: Tactic: 6 time 45.706
[TensorRT] VERBOSE: Tactic: 57 time 0.319063
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.319063
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: Tactic: -1 time 0.365781
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.365781
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaConvolution Tactic: 57
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,47040), Float(1,7,49,7840) -> Float(1,7,49,7840) ***************
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.320833
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.334688
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.275365
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.330313
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.28
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.283229
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.303281
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.338906
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.268021
[TensorRT] VERBOSE: Fastest Tactic: -37215280111360163 Time: 0.268021
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.347396
[TensorRT] VERBOSE: Tactic: 2 time 0.60375
[TensorRT] VERBOSE: Tactic: 5 time 13.8324
[TensorRT] VERBOSE: Tactic: 57 time 0.311823
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.311823
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -37215280111360163
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,7840) -> Float(1,7,49,47040) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,47040) -> Float(1,7,49,47040) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,47040), Float(1,7,49,7840) -> Float(1,7,49,7840) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,7840) -> Float(1,7,49,47040) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,47040) -> Float(1,7,49,47040) ***************
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,47040) -> Float(1,7,49,15680) ***************
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: Tactic: 589823 time 0.6925
[TensorRT] VERBOSE: Tactic: 655359 time 0.784427
[TensorRT] VERBOSE: Tactic: 786431 time 0.749896
[TensorRT] VERBOSE: Tactic: 851967 time 0.699011
[TensorRT] VERBOSE: Tactic: 1179647 time 0.55375
[TensorRT] VERBOSE: Tactic: 1310719 time 1.11401
[TensorRT] VERBOSE: Tactic: 1376255 time 0.446302
[TensorRT] VERBOSE: Tactic: 1441791 time 0.742239
[TensorRT] VERBOSE: Tactic: 1507327 time 0.675833
[TensorRT] VERBOSE: Tactic: 1638399 time 0.843907
[TensorRT] VERBOSE: Tactic: 1835007 time 0.580364
[TensorRT] VERBOSE: Tactic: 1900543 time 0.861719
[TensorRT] VERBOSE: Tactic: 2097151 time 0.504115
[TensorRT] VERBOSE: Tactic: 2162687 time 0.513541
[TensorRT] VERBOSE: Tactic: 2293759 time 0.441979
[TensorRT] VERBOSE: Tactic: 2359295 time 0.554323
[TensorRT] VERBOSE: Tactic: 2686975 time 0.449427
[TensorRT] VERBOSE: Tactic: 3080191 time 0.621146
[TensorRT] VERBOSE: Tactic: 3342335 time 0.904687
[TensorRT] VERBOSE: Tactic: 3407871 time 0.503177
[TensorRT] VERBOSE: Tactic: 3538943 time 0.464375
[TensorRT] VERBOSE: Tactic: 3670015 time 0.775521
[TensorRT] VERBOSE: Tactic: 3932159 time 0.708646
[TensorRT] VERBOSE: Tactic: 3997695 time 0.756771
[TensorRT] VERBOSE: Tactic: 4063231 time 0.599062
[TensorRT] VERBOSE: Tactic: 4194303 time 0.408646
[TensorRT] VERBOSE: Tactic: 4259839 time 0.515781
[TensorRT] VERBOSE: Tactic: 4325375 time 0.700104
[TensorRT] VERBOSE: Tactic: 4521983 time 0.921562
[TensorRT] VERBOSE: Tactic: 4587519 time 0.612344
[TensorRT] VERBOSE: Tactic: 4653055 time 0.570156
[TensorRT] VERBOSE: Tactic: 4915199 time 0.410937
[TensorRT] VERBOSE: Tactic: 4980735 time 0.676875
[TensorRT] VERBOSE: Tactic: 5177343 time 0.54401
[TensorRT] VERBOSE: Tactic: 5242879 time 0.470468
[TensorRT] VERBOSE: Tactic: 5373951 time 0.539323
[TensorRT] VERBOSE: Tactic: 5439487 time 0.517813
[TensorRT] VERBOSE: Tactic: 5570559 time 0.461041
[TensorRT] VERBOSE: Tactic: 5636095 time 0.577187
[TensorRT] VERBOSE: Tactic: 5701631 time 0.432396
[TensorRT] VERBOSE: Tactic: 5767167 time 0.736042
[TensorRT] VERBOSE: Tactic: 5832703 time 0.483906
[TensorRT] VERBOSE: Tactic: 5898239 time 0.391667
[TensorRT] VERBOSE: Tactic: 6029311 time 0.402761
[TensorRT] VERBOSE: Tactic: 6225919 time 0.445468
[TensorRT] VERBOSE: Tactic: 6291455 time 0.544792
[TensorRT] VERBOSE: Tactic: 6422527 time 0.421927
[TensorRT] VERBOSE: Tactic: 6750207 time 0.533854
[TensorRT] VERBOSE: Tactic: 6815743 time 0.459167
[TensorRT] VERBOSE: Tactic: 6946815 time 0.738959
[TensorRT] VERBOSE: Tactic: 7012351 time 0.499844
[TensorRT] VERBOSE: Tactic: 7077887 time 0.454948
[TensorRT] VERBOSE: Tactic: 7143423 time 0.789531
[TensorRT] VERBOSE: Tactic: 7208959 time 0.477188
[TensorRT] VERBOSE: Tactic: 7340031 time 0.372812
[TensorRT] VERBOSE: Tactic: 7405567 time 0.55401
[TensorRT] VERBOSE: Tactic: 7536639 time 0.676927
[TensorRT] VERBOSE: Tactic: 7602175 time 0.730729
[TensorRT] VERBOSE: Tactic: 7733247 time 0.406562
[TensorRT] VERBOSE: Tactic: 7798783 time 0.750834
[TensorRT] VERBOSE: Tactic: 8191999 time 0.73349
[TensorRT] VERBOSE: Tactic: 8257535 time 0.418802
[TensorRT] VERBOSE: Tactic: 8323071 time 0.517136
[TensorRT] VERBOSE: Tactic: 8650751 time 0.754896
[TensorRT] VERBOSE: Tactic: 8716287 time 0.443907
[TensorRT] VERBOSE: Tactic: 9109503 time 0.507292
[TensorRT] VERBOSE: Tactic: 9568255 time 0.412395
[TensorRT] VERBOSE: Tactic: 9895935 time 0.407761
[TensorRT] VERBOSE: Tactic: 10223615 time 0.450625
[TensorRT] VERBOSE: Tactic: 10354687 time 0.518645
[TensorRT] VERBOSE: Tactic: 10551295 time 0.51401
[TensorRT] VERBOSE: Tactic: 10747903 time 0.403958
[TensorRT] VERBOSE: Tactic: 10944511 time 0.662135
[TensorRT] VERBOSE: Fastest Tactic: 7340031 Time: 0.372812
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.544427
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.53276
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.489479
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.495
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.513541
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.498906
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.518333
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.516718
[TensorRT] VERBOSE: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.488281
[TensorRT] VERBOSE: Fastest Tactic: -37215280111360163 Time: 0.488281
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.620312
[TensorRT] VERBOSE: Tactic: 2 time 0.794271
[TensorRT] VERBOSE: Tactic: 5 skipped. Scratch requested: 42997760, available: 33554432
[TensorRT] VERBOSE: Tactic: 57 time 0.487708
[TensorRT] INFO: Some tactics do not have sufficient workspace memory to run. Increasing workspace size may increase performance, please check verbose output.
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.487708
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: FusedConvActConvolution Tactic: 7340031
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,15680) -> Float(1,7,49,62720) ***************
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (FusedConvActConvolution)
[TensorRT] VERBOSE: FusedConvActConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (CaskConvolution)
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 1062367460111450758 time 0.77625
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 4501471010995462441 time 0.60125
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: 5137655947464784826 time 0.590416
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: 5326823351883942011 time 0.581927
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: Tactic: 6645123197870846056 time 0.602761
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -6576203419454146580 time 0.668125
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -3456450830548107839 time 0.725573
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Tactic: -410470605513481746 time 0.57901
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: Tactic: -37215280111360163 time 0.583177
[TensorRT] VERBOSE: Fastest Tactic: -410470605513481746 Time: 0.57901
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (CudaConvolution)
[TensorRT] VERBOSE: Tactic: 0 time 0.913541
[TensorRT] VERBOSE: Tactic: 2 time 0.94599
[TensorRT] VERBOSE: Tactic: 5 skipped. Scratch requested: 57228800, available: 33554432
[TensorRT] VERBOSE: Tactic: 57 time 0.769895
[TensorRT] VERBOSE: Fastest Tactic: 57 Time: 0.769895
[TensorRT] VERBOSE: --------------- Timing Runner: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (CudaDepthwiseConvolution)
[TensorRT] VERBOSE: CudaDepthwiseConvolution has no valid tactics for this config, skipping
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CaskConvolution Tactic: -410470605513481746
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,7,49,62720) -> Float(1,1,1,1280) ***************
[TensorRT] VERBOSE: --------------- Timing Runner: [AVERAGE #1] torch.nn.functional.adaptive_avg_pool2d(AdaptiveAvgPool2d(output_size=1), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (Pooling)
[TensorRT] VERBOSE: Tactic: -1 time 0.026979
[TensorRT] VERBOSE: Fastest Tactic: -1 Time: 0.026979
[TensorRT] VERBOSE: --------------- Timing Runner: [AVERAGE #1] torch.nn.functional.adaptive_avg_pool2d(AdaptiveAvgPool2d(output_size=1), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (TiledPooling)
[TensorRT] VERBOSE: Tactic: 7995649 time 0.207552
[TensorRT] VERBOSE: Fastest Tactic: 7995649 Time: 0.207552
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: Pooling Tactic: -1
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,1,1,1280) -> Float(1,1,1,1000) ***************
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_128x128_relu_nn_v1
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_128x64_relu_nn_v1
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_64x64_relu_nn_v1
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_32x128_relu_nn_v1
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_128x32_relu_nn_v1
[TensorRT] VERBOSE: --------------- Timing Runner: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (CaskFullyConnected)
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_128x128_relu_nn_v1
[TensorRT] VERBOSE: Tactic: 8883888914904656451 time 1.60203
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_128x64_relu_nn_v1
[TensorRT] VERBOSE: Tactic: 5453137127347942357 time 0.816979
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_64x64_relu_nn_v1
[TensorRT] VERBOSE: Tactic: 5373503982740029499 time 0.919114
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_32x128_relu_nn_v1
[TensorRT] VERBOSE: Tactic: 4133936625481774016 time 3.45922
[TensorRT] VERBOSE: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (caskFullyConnectedFP32) Set Tactic Name: maxwell_sgemm_128x32_relu_nn_v1
[TensorRT] VERBOSE: Tactic: 1933552664043962183 time 0.526406
[TensorRT] VERBOSE: Fastest Tactic: 1933552664043962183 Time: 0.526406
[TensorRT] VERBOSE: --------------- Timing Runner: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (CudaFullyConnected)
[TensorRT] VERBOSE: Tactic: 0 time 0.383594
[TensorRT] VERBOSE: Tactic: 1 time 0.470886
[TensorRT] VERBOSE: Tactic: 9 time 0.516823
[TensorRT] VERBOSE: Tactic: 26 time 0.49125
[TensorRT] VERBOSE: Tactic: 27 time 0.532552
[TensorRT] VERBOSE: Tactic: 48 time 0.425052
[TensorRT] VERBOSE: Tactic: 49 time 0.478177
[TensorRT] VERBOSE: Fastest Tactic: 0 Time: 0.383594
[TensorRT] VERBOSE: >>>>>>>>>>>>>>> Chose Runner Type: CudaFullyConnected Tactic: 0
[TensorRT] VERBOSE: 
[TensorRT] VERBOSE: *************** Autotuning format combination: Float(1,1,1,1000) -> Float(1,1000) ***************
[TensorRT] VERBOSE: --------------- Timing Runner: [SHUFFLE #38] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) (Shuffle)
[TensorRT] VERBOSE: Tactic: 0 time 0.003542
[TensorRT] VERBOSE: Tactic: 1 time 0.010989
[TensorRT] VERBOSE: Fastest Tactic: 0 Time: 0.003542
[TensorRT] VERBOSE: Formats and tactics selection completed in 37.2866 seconds.
[TensorRT] VERBOSE: After reformat layers: 55 layers
[TensorRT] VERBOSE: Block size 33554432
[TensorRT] VERBOSE: Block size 4816896
[TensorRT] VERBOSE: Block size 1806336
[TensorRT] VERBOSE: Block size 301056
[TensorRT] VERBOSE: Block size 100352
[TensorRT] VERBOSE: Total Activation Memory: 40579072
[TensorRT] INFO: Detected 1 inputs and 1 output network tensors.
[TensorRT] VERBOSE: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #10] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #16] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #18] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #15] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #19] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_small_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #25] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #27] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #23] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #28] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #30] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #26] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #31] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #37] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #39] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #34] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x32_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #40] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_medium_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #46] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #40] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #48] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #42] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #49] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #43] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x64_relu_interior_nn_v1
[TensorRT] VERBOSE: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) (scudnn) Set Tactic Name: maxwell_scudnn_128x128_relu_small_nn_v1
[TensorRT] VERBOSE: Layer: [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 79360
[TensorRT] VERBOSE: Layer: [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) Weights: 1152 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 77824
[TensorRT] VERBOSE: Layer: [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 82432
[TensorRT] VERBOSE: Layer: [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) Weights: 3456 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) Weights: 0 HostPersistent: 2176 DevicePersistent: 28672
[TensorRT] VERBOSE: Layer: [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 33792
[TensorRT] VERBOSE: Layer: [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) Weights: 5184 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 33280
[TensorRT] VERBOSE: Layer: [CONVOLUTION #10] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 33792
[TensorRT] VERBOSE: Layer: [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) Weights: 5184 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 23552
[TensorRT] VERBOSE: Layer: [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 30208
[TensorRT] VERBOSE: Layer: [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) Weights: 6912 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 29696
[TensorRT] VERBOSE: Layer: [CONVOLUTION #16] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 30208
[TensorRT] VERBOSE: Layer: [CONVOLUTION #17] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #12] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) Weights: 6912 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #18] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #15] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 29696
[TensorRT] VERBOSE: Layer: [CONVOLUTION #19] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 30208
[TensorRT] VERBOSE: Layer: [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) Weights: 6912 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 50688
[TensorRT] VERBOSE: Layer: [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 101376
[TensorRT] VERBOSE: Layer: [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 13824 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 99840
[TensorRT] VERBOSE: Layer: [CONVOLUTION #25] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 101376
[TensorRT] VERBOSE: Layer: [CONVOLUTION #26] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 13824 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #27] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #23] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 99840
[TensorRT] VERBOSE: Layer: [CONVOLUTION #28] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 101376
[TensorRT] VERBOSE: Layer: [CONVOLUTION #29] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #20] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 13824 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #30] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #26] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 99840
[TensorRT] VERBOSE: Layer: [CONVOLUTION #31] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 101376
[TensorRT] VERBOSE: Layer: [CONVOLUTION #32] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 13824 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 149504
[TensorRT] VERBOSE: Layer: [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 2176 DevicePersistent: 224768
[TensorRT] VERBOSE: Layer: [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) Weights: 20736 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 223232
[TensorRT] VERBOSE: Layer: [CONVOLUTION #37] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 2176 DevicePersistent: 224768
[TensorRT] VERBOSE: Layer: [CONVOLUTION #38] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #26] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) Weights: 20736 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #39] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #34] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 223232
[TensorRT] VERBOSE: Layer: [CONVOLUTION #40] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) Weights: 0 HostPersistent: 2176 DevicePersistent: 224768
[TensorRT] VERBOSE: Layer: [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) Weights: 20736 HostPersistent: 8 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) Weights: 368640 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 619008
[TensorRT] VERBOSE: Layer: [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 34560 HostPersistent: 8 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 615936
[TensorRT] VERBOSE: Layer: [CONVOLUTION #46] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #40] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 619008
[TensorRT] VERBOSE: Layer: [CONVOLUTION #47] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #41] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 34560 HostPersistent: 8 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #48] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #42] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 615936
[TensorRT] VERBOSE: Layer: [CONVOLUTION #49] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #43] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 3200 DevicePersistent: 619008
[TensorRT] VERBOSE: Layer: [CONVOLUTION #50] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #34] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #44] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 34560 HostPersistent: 8 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) Weights: 1228800 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 1664 DevicePersistent: 1644032
[TensorRT] VERBOSE: Layer: [AVERAGE #1] torch.nn.functional.adaptive_avg_pool2d(AdaptiveAvgPool2d(output_size=1), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) Weights: 0 HostPersistent: 16 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) Weights: 5120000 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Layer: [SHUFFLE #38] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)) Weights: 0 HostPersistent: 0 DevicePersistent: 0
[TensorRT] VERBOSE: Total Host Persistent Memory: 87728
[TensorRT] VERBOSE: Total Device Persistent Memory: 7301632
[TensorRT] VERBOSE: Total Weight Memory: 6974336
[TensorRT] VERBOSE: Builder timing cache: created 31 entries, 22 hit(s)
[TensorRT] VERBOSE: Engine generation completed in 43.2647 seconds.
[TensorRT] VERBOSE: Engine Layer Information:
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #1] torch.nn.Conv2d.forward(Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False), tensor(shape=[1, 3, 224, 224], dtype=torch.float32)) + [RELU #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #1] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)), Tactic: -3456450830548107839, input_0[Float(3,224,224)] -> (Unnamed Layer* 5) [ElementWise]_output[Float(32,112,112)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #2] torch.nn.Conv2d.forward(Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [RELU #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #2] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 5) [ElementWise]_output[Float(32,112,112)] -> (Unnamed Layer* 11) [ElementWise]_output[Float(32,112,112)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #3] torch.nn.Conv2d.forward(Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 112, 112], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 11) [ElementWise]_output[Float(32,112,112)] -> (Unnamed Layer* 13) [Scale]_output[Float(16,112,112)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #4] torch.nn.Conv2d.forward(Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 16, 112, 112], dtype=torch.float32)) + [RELU #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [ELEMENTWISE #3] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)), Tactic: 5137655947464784826, (Unnamed Layer* 13) [Scale]_output[Float(16,112,112)] -> (Unnamed Layer* 19) [ElementWise]_output[Float(96,112,112)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #5] torch.nn.Conv2d.forward(Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False), tensor(shape=[1, 96, 112, 112], dtype=torch.float32)) + [RELU #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #4] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 19) [ElementWise]_output[Float(96,112,112)] -> (Unnamed Layer* 25) [ElementWise]_output[Float(96,56,56)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #6] torch.nn.Conv2d.forward(Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 56, 56], dtype=torch.float32)), Tactic: 1062367460111450758, (Unnamed Layer* 25) [ElementWise]_output[Float(96,56,56)] -> (Unnamed Layer* 27) [Scale]_output[Float(24,56,56)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #7] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #5] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)), Tactic: 5137655947464784826, (Unnamed Layer* 27) [Scale]_output[Float(24,56,56)] -> (Unnamed Layer* 33) [ElementWise]_output[Float(144,56,56)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #8] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #6] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 33) [ElementWise]_output[Float(144,56,56)] -> (Unnamed Layer* 39) [ElementWise]_output[Float(144,56,56)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #9] torch.nn.Conv2d.forward(Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #7] torch.Tensor.__add__(tensor(shape=[1, 24, 56, 56], dtype=torch.float32), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 39) [ElementWise]_output[Float(144,56,56)], (Unnamed Layer* 27) [Scale]_output[Float(24,56,56)] -> (Unnamed Layer* 42) [ElementWise]_output[Float(24,56,56)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #10] torch.nn.Conv2d.forward(Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 24, 56, 56], dtype=torch.float32)) + [RELU #7] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [ELEMENTWISE #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)), Tactic: 5137655947464784826, (Unnamed Layer* 42) [ElementWise]_output[Float(24,56,56)] -> (Unnamed Layer* 48) [ElementWise]_output[Float(144,56,56)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #11] torch.nn.Conv2d.forward(Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False), tensor(shape=[1, 144, 56, 56], dtype=torch.float32)) + [RELU #8] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 48) [ElementWise]_output[Float(144,56,56)] -> (Unnamed Layer* 54) [ElementWise]_output[Float(144,28,28)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #12] torch.nn.Conv2d.forward(Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 144, 28, 28], dtype=torch.float32)), Tactic: -3456450830548107839, (Unnamed Layer* 54) [ElementWise]_output[Float(144,28,28)] -> (Unnamed Layer* 56) [Scale]_output[Float(32,28,28)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #13] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #9] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)), Tactic: 5137655947464784826, (Unnamed Layer* 56) [Scale]_output[Float(32,28,28)] -> (Unnamed Layer* 62) [ElementWise]_output[Float(192,28,28)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #14] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #10] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 62) [ElementWise]_output[Float(192,28,28)] -> (Unnamed Layer* 68) [ElementWise]_output[Float(192,28,28)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #15] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #12] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 68) [ElementWise]_output[Float(192,28,28)], (Unnamed Layer* 56) [Scale]_output[Float(32,28,28)] -> (Unnamed Layer* 71) [ElementWise]_output[Float(32,28,28)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #16] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #11] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)), Tactic: 5137655947464784826, (Unnamed Layer* 71) [ElementWise]_output[Float(32,28,28)] -> (Unnamed Layer* 77) [ElementWise]_output[Float(192,28,28)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #17] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #12] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 77) [ElementWise]_output[Float(192,28,28)] -> (Unnamed Layer* 83) [ElementWise]_output[Float(192,28,28)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #18] torch.nn.Conv2d.forward(Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #15] torch.Tensor.__add__(tensor(shape=[1, 32, 28, 28], dtype=torch.float32), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 83) [ElementWise]_output[Float(192,28,28)], (Unnamed Layer* 71) [ElementWise]_output[Float(32,28,28)] -> (Unnamed Layer* 86) [ElementWise]_output[Float(32,28,28)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #19] torch.nn.Conv2d.forward(Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 32, 28, 28], dtype=torch.float32)) + [RELU #13] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [ELEMENTWISE #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)), Tactic: 5137655947464784826, (Unnamed Layer* 86) [ElementWise]_output[Float(32,28,28)] -> (Unnamed Layer* 92) [ElementWise]_output[Float(192,28,28)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #20] torch.nn.Conv2d.forward(Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False), tensor(shape=[1, 192, 28, 28], dtype=torch.float32)) + [RELU #14] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 92) [ElementWise]_output[Float(192,28,28)] -> (Unnamed Layer* 98) [ElementWise]_output[Float(192,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #21] torch.nn.Conv2d.forward(Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 192, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 98) [ElementWise]_output[Float(192,14,14)] -> (Unnamed Layer* 100) [Scale]_output[Float(64,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #22] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #15] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 100) [Scale]_output[Float(64,14,14)] -> (Unnamed Layer* 106) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #23] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #16] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 106) [ElementWise]_output[Float(384,14,14)] -> (Unnamed Layer* 112) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #24] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #20] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 112) [ElementWise]_output[Float(384,14,14)], (Unnamed Layer* 100) [Scale]_output[Float(64,14,14)] -> (Unnamed Layer* 115) [ElementWise]_output[Float(64,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #25] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #17] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 115) [ElementWise]_output[Float(64,14,14)] -> (Unnamed Layer* 121) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #26] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #18] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 121) [ElementWise]_output[Float(384,14,14)] -> (Unnamed Layer* 127) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #27] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #23] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 127) [ElementWise]_output[Float(384,14,14)], (Unnamed Layer* 115) [ElementWise]_output[Float(64,14,14)] -> (Unnamed Layer* 130) [ElementWise]_output[Float(64,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #28] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #19] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 130) [ElementWise]_output[Float(64,14,14)] -> (Unnamed Layer* 136) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #29] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #20] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 136) [ElementWise]_output[Float(384,14,14)] -> (Unnamed Layer* 142) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #30] torch.nn.Conv2d.forward(Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #26] torch.Tensor.__add__(tensor(shape=[1, 64, 14, 14], dtype=torch.float32), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 142) [ElementWise]_output[Float(384,14,14)], (Unnamed Layer* 130) [ElementWise]_output[Float(64,14,14)] -> (Unnamed Layer* 145) [ElementWise]_output[Float(64,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #31] torch.nn.Conv2d.forward(Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 64, 14, 14], dtype=torch.float32)) + [RELU #21] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 145) [ElementWise]_output[Float(64,14,14)] -> (Unnamed Layer* 151) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #32] torch.nn.Conv2d.forward(Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [RELU #22] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 151) [ElementWise]_output[Float(384,14,14)] -> (Unnamed Layer* 157) [ElementWise]_output[Float(384,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #33] torch.nn.Conv2d.forward(Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 384, 14, 14], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 157) [ElementWise]_output[Float(384,14,14)] -> (Unnamed Layer* 159) [Scale]_output[Float(96,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #34] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #23] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)), Tactic: 6645123197870846056, (Unnamed Layer* 159) [Scale]_output[Float(96,14,14)] -> (Unnamed Layer* 165) [ElementWise]_output[Float(576,14,14)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #35] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #24] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 165) [ElementWise]_output[Float(576,14,14)] -> (Unnamed Layer* 171) [ElementWise]_output[Float(576,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #36] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #31] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 171) [ElementWise]_output[Float(576,14,14)], (Unnamed Layer* 159) [Scale]_output[Float(96,14,14)] -> (Unnamed Layer* 174) [ElementWise]_output[Float(96,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #37] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #25] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)), Tactic: 6645123197870846056, (Unnamed Layer* 174) [ElementWise]_output[Float(96,14,14)] -> (Unnamed Layer* 180) [ElementWise]_output[Float(576,14,14)]
[TensorRT] VERBOSE: Layer(DepthwiseConvolution): [CONVOLUTION #38] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #26] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 180) [ElementWise]_output[Float(576,14,14)] -> (Unnamed Layer* 186) [ElementWise]_output[Float(576,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #39] torch.nn.Conv2d.forward(Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #34] torch.Tensor.__add__(tensor(shape=[1, 96, 14, 14], dtype=torch.float32), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)), Tactic: -6576203419454146580, (Unnamed Layer* 186) [ElementWise]_output[Float(576,14,14)], (Unnamed Layer* 174) [ElementWise]_output[Float(96,14,14)] -> (Unnamed Layer* 189) [ElementWise]_output[Float(96,14,14)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #40] torch.nn.Conv2d.forward(Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 96, 14, 14], dtype=torch.float32)) + [RELU #27] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [ELEMENTWISE #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)), Tactic: 6645123197870846056, (Unnamed Layer* 189) [ElementWise]_output[Float(96,14,14)] -> (Unnamed Layer* 195) [ElementWise]_output[Float(576,14,14)]
[TensorRT] VERBOSE: Layer(Convolution): [CONVOLUTION #41] torch.nn.Conv2d.forward(Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False), tensor(shape=[1, 576, 14, 14], dtype=torch.float32)) + [RELU #28] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #36] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)), Tactic: 57, (Unnamed Layer* 195) [ElementWise]_output[Float(576,14,14)] -> (Unnamed Layer* 201) [ElementWise]_output[Float(576,7,7)]
[TensorRT] VERBOSE: Layer(FusedConvActDirect): [CONVOLUTION #42] torch.nn.Conv2d.forward(Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 576, 7, 7], dtype=torch.float32)), Tactic: 5898239, (Unnamed Layer* 201) [ElementWise]_output[Float(576,7,7)] -> (Unnamed Layer* 203) [Scale]_output[Float(160,7,7)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #43] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #29] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #37] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 203) [Scale]_output[Float(160,7,7)] -> (Unnamed Layer* 209) [ElementWise]_output[Float(960,7,7)]
[TensorRT] VERBOSE: Layer(Convolution): [CONVOLUTION #44] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #30] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #38] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: 57, (Unnamed Layer* 209) [ElementWise]_output[Float(960,7,7)] -> (Unnamed Layer* 215) [ElementWise]_output[Float(960,7,7)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #45] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #39] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 215) [ElementWise]_output[Float(960,7,7)], (Unnamed Layer* 203) [Scale]_output[Float(160,7,7)] -> (Unnamed Layer* 218) [ElementWise]_output[Float(160,7,7)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #46] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #31] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #40] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 218) [ElementWise]_output[Float(160,7,7)] -> (Unnamed Layer* 224) [ElementWise]_output[Float(960,7,7)]
[TensorRT] VERBOSE: Layer(Convolution): [CONVOLUTION #47] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #32] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #41] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: 57, (Unnamed Layer* 224) [ElementWise]_output[Float(960,7,7)] -> (Unnamed Layer* 230) [ElementWise]_output[Float(960,7,7)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #48] torch.nn.Conv2d.forward(Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #42] torch.Tensor.__add__(tensor(shape=[1, 160, 7, 7], dtype=torch.float32), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 230) [ElementWise]_output[Float(960,7,7)], (Unnamed Layer* 218) [ElementWise]_output[Float(160,7,7)] -> (Unnamed Layer* 233) [ElementWise]_output[Float(160,7,7)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #49] torch.nn.Conv2d.forward(Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 160, 7, 7], dtype=torch.float32)) + [RELU #33] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #43] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: -37215280111360163, (Unnamed Layer* 233) [ElementWise]_output[Float(160,7,7)] -> (Unnamed Layer* 239) [ElementWise]_output[Float(960,7,7)]
[TensorRT] VERBOSE: Layer(Convolution): [CONVOLUTION #50] torch.nn.Conv2d.forward(Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [RELU #34] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #44] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: 57, (Unnamed Layer* 239) [ElementWise]_output[Float(960,7,7)] -> (Unnamed Layer* 245) [ElementWise]_output[Float(960,7,7)]
[TensorRT] VERBOSE: Layer(FusedConvActDirect): [CONVOLUTION #51] torch.nn.Conv2d.forward(Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 960, 7, 7], dtype=torch.float32)), Tactic: 7340031, (Unnamed Layer* 245) [ElementWise]_output[Float(960,7,7)] -> (Unnamed Layer* 247) [Scale]_output[Float(320,7,7)]
[TensorRT] VERBOSE: Layer(scudnn): [CONVOLUTION #52] torch.nn.Conv2d.forward(Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False), tensor(shape=[1, 320, 7, 7], dtype=torch.float32)) + [RELU #35] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)) + [ELEMENTWISE #45] torch.nn.ReLU6.forward(ReLU6(inplace=True), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)), Tactic: -410470605513481746, (Unnamed Layer* 247) [Scale]_output[Float(320,7,7)] -> (Unnamed Layer* 253) [ElementWise]_output[Float(1280,7,7)]
[TensorRT] VERBOSE: Layer(Pooling): [AVERAGE #1] torch.nn.functional.adaptive_avg_pool2d(AdaptiveAvgPool2d(output_size=1), tensor(shape=[1, 1280, 7, 7], dtype=torch.float32)), Tactic: -1, (Unnamed Layer* 253) [ElementWise]_output[Float(1280,7,7)] -> (Unnamed Layer* 254) [Pooling]_output[Float(1280,1,1)]
[TensorRT] VERBOSE: Layer(FullyConnected): [FULLY_CONNECTED #1] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)), Tactic: 0, (Unnamed Layer* 254) [Pooling]_output[Float(1280,1,1)] -> (Unnamed Layer* 257) [Fully Connected]_output[Float(1000,1,1)]
[TensorRT] VERBOSE: Layer(Shuffle): [SHUFFLE #38] torch.nn.Linear.forward(Linear(in_features=1280, out_features=1000, bias=True), tensor(shape=[1, 1280], dtype=torch.float32)), Tactic: 0, (Unnamed Layer* 257) [Fully Connected]_output[Float(1000,1,1)] -> output_0[Float(1000)]
orange 0.9588285684585571
lemon 0.018653737381100655
jack-o'-lantern 0.010776862502098083
butternut squash 0.003165015485137701
pot, flowerpot 0.0016122423112392426
orange 0.9588284492492676
lemon 0.018653661012649536
jack-o'-lantern 0.01077687181532383
butternut squash 0.0031650271266698837
pot, flowerpot 0.0016122451052069664
tensor(1.4663e-05, device='cuda:0')
]0;root@agora-desktop: /jetson-inference/temp/lab3root@agora-desktop:/jetson-inference/temp/lab3# eixt   xit
exit

Script done on 2021-05-15 14:04:23+0000
